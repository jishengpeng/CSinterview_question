# CSinterview_question
### 记录一些计算机保研面试中老师问的和自己准备的一些专业性的问题，关于项目的/一些家常的/一些专业性很低的问题除外。给自己查漏补缺用，正常后缀表示学院，p表示是自己准备的问题。


#### 1.贪心算法一定取到最优解吗？（hit）
答：不是，只有当问题满足最优子结构（当一个问题的子问题，贪心的选择比非贪心的选择更好）的时候才满足。（老师好像要的是给出一种通用的证明，我觉得得具体问题具体看。）
#### 2.分析一下深度可分离卷积（p）
答：首先理解一下torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)是怎么通过一个普通的卷积操作来改变信道数的。这里的输出信道数其实就是卷积核的个数，一个彩色图像的卷积核其实是三维立体的，而不是单纯二维的。例如一个一百通道的图片，经过一个尺寸3\*3的卷积核，实际上是经过一个3\*3\*100的卷积核，输入100张信道分别和“100张（100个信道，其实100加起来是一个卷积核）”3*3卷积核相乘，再融合变成一张特征图。这个特征图（此时只有1信道）。所以多少个卷积核就有多少个特征图，就可以输出多少信道。深度可分离卷积其实为了（模型压缩使用的，精度换参数量）。他由两部分构成，空间分离卷积和点卷积。空间分离卷积中的卷积核是二维的，（可以理解为卷积核的信道数是1）。并且空间分离卷积中的卷积核数量必须等于输入图片的信道数，这样保证了输出输入信道数一样（注意此时没有普通卷积中的融合过程）。但是这时候有个很大的问题是一个图像中的各个信道之间信息交互并没有学习到，所有后面加了一个点卷积，点卷积其实是一个（普通的三维！）1\*1卷积核。他会有融合过程，既最大程度保留信息又解决了之前所说的融合问题，而且既然是普通的三维卷积，还可以通过卷积核的数目来改变输出信道。
#### 3.分析一下空洞卷积（p，更大感受野，添0）
#### 4.分析一下各种归一化，batch norm，layer norm，instance norm，group norm（p）
答：批归一化是将同一类信道（比如红色）一个batch_size中所有图片矩阵进行归一化，层归一化是一张图片所有信道，IN是一张图片一个信道，GN是介于LN和IN之间。
#### 5.分析一下准确率，精准率，召回率，F1-score（p）
答：理解TP，FP，TN，FN的概念（TF表示预测结果是否正确，P表示预测的结果是正样本（实际可正可负），F反之）。准确率就是预测正确占所有样本的比例。精准率是预测为正样本且预测正确占预测为正样本的比例（多预测了）。召回率是预测为正样本且预测正确占实际为正样本的比例（少预测了）。F1-score是精准率和召回率的调和平均。
#### 6.分析一下直接插入排序，希尔排序，冒泡排序，快速排序，简单选择排序，堆排序，归并排序（p稳定性，时间复杂度，快排和堆排序都是不稳定且为O（nlogn）的，手撕快排代码）
