# CSinterview_question
## 记录一些计算机保研面试中老师问的和自己准备的一些专业性的问题，关于项目的/一些家常的/一些专业性很低的问题就不谈了。给自己查漏补缺用，正常后缀表示学院，p表示是自己准备的问题,ps表示问题自己准备且简单叙述。


#### 1.贪心算法一定取到最优解吗？（hit）
答：不是，只有当问题满足最优子结构（当一个问题的子问题，贪心的选择比非贪心的选择更好）的时候才满足。（老师好像要的是给出一种通用的证明，我觉得得具体问题具体看。）
#### 2.分析一下深度可分离卷积（p）
答：首先理解一下torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)是怎么通过一个普通的卷积操作来改变信道数的。这里的输出信道数其实就是卷积核的个数，一个彩色图像的卷积核其实是三维立体的，而不是单纯二维的。例如一个一百通道的图片，经过一个尺寸3\*3的卷积核，实际上是经过一个3\*3\*100的卷积核，100张信道分别和“100张（100个信道，其实100加起来是一个卷积核）”3*3卷积核相乘，再融合变成一张特征图。这个特征图（此时只有1信道）。所以多少个卷积核就有多少个特征图，就可以输出多少信道。深度可分离卷积其实为了（模型压缩使用的，精度换参数量）。他由两部分构成，空间分离卷积和点卷积。空间分离卷积中的卷积核是二维的，（可以理解为卷积核的信道数是1）。并且空间分离卷积中的卷积核数量必须等于输入图片的信道数，这样保证了输出输入信道数一样（注意此时没有普通卷积中的融合过程）。但是这时候有个很大的问题是一个图像中的各个信道之间信息交互并没有学习到，所有后面加了一个点卷积，点卷积其实是一个（普通的三维！）1\*1卷积核。他会有融合过程，既最大程度保留信息又解决了之前所说的融合问题，而且既然是普通的三维卷积，还可以通过卷积核的数目来改变输出信道。
#### 3.分析一下空洞卷积（p易，更大感受野，添0）
#### 4.分析一下各种归一化，batch norm，layer norm，instance norm，group norm（p）
答：批归一化是将同一类信道（比如红色）一个batch_size中所有图片矩阵进行归一化，层归一化是一张图片所有信道，IN是一张图片一个信道，GN是介于LN和IN之间。
#### 